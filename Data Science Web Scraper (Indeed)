import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from urllib.request import urlopen
from bs4 import BeautifulSoup
import re


def extract(page):
    header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'}
    url = f'https://www.indeed.com/jobs?q=data%20science&l=Washington%2C%20DC&start={page}'
    r = requests.get(url, header)
    soup = BeautifulSoup(r.content, 'html.parser')
    return soup

def transform(soup):
    divs = soup.find_all('div', class_ = 'slider_container')
    for job in divs:
        title = job.find('div').text.strip()
        company = job.find('span', class_ = 'companyName').text.strip()
        location = job.find('div', class_ = 'companyLocation').text.strip()
        link = job.find('a', class_ = 'href')
        summary = job.find('div', class_ = 'job-snippet').text.strip().replace('\n', '')
        
        job = {'title': title, 
               'company': company,
               'location': location,
               'link': link,
               'summary': summary,
            
        }
        joblist.append(job)
    return


joblist = []

for i in range(0, 40, 10):
    print (f'Running page, {i}')
    c = extract(0)
    transform(c)
    
df = pd.DataFrame(joblist)
print (df.head())
df.to_csv('job.csv')
